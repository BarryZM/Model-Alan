# 介绍
样例文件是聚类数目为7的朴素贝叶斯非监督实现，聚类主题数改变直接改变code中K即可。

# 运行环境
• Core: 4 * Intel(R) Core(TM) i5-3317U CPU @ 1.70GHz
• OS: Linux 4.8.4-1-ARCH #1 SMP PREEMPT x86_64 GNU/Linux
• Language: Python 2.7
• Data Set: Activity Recognition from a Single Chest-Mounted Accelerometer
# 问题描述
对输入数据 [X1, X2...Xn ] 进行聚类。注:每个 Xi 表示一个 L 维度的样本,样本个数为 N。使用 聚类结果与原有数据类别做比较,查看聚类效果。
1. 输入数据
注:我们将数据集切分,0.8 进行训练,0.2 进行测试 2. 期望输出
(1)对于每个样本聚类的结果; (2)与原有数据类别相比的聚类效果(训练集与测试集)。
X1 =[x11,x12...x1L] X2 =[x21,x22...x2L] ...
XN =[xN1,xN2...xNL]
1
第一部分:对原有数据进行聚类(使用 EM 算法求解)
对输入数据 [X1, X2...Xn ] 进行聚类。注:每个 Xi 表示一个 L 维度的样本,样本个数为 N。使用
聚类结果与原有数据类别做比较,查看聚类效果。 1. 假设
(1)模型生成过程:假设有 k 个类别,对于每一样本的生成过程:首先从 k 个类别中选择一个, 之后在该类别下独立分别选择此样本的各个维度数据,构成当前样本。
(2)样本之间相互独立;各样本各特征之间相互独立。 2. 目标
使得生成模型概率最大。
3. EM 算法求解
a) 明确隐变量
X(i) 表示第 i 个样本;
隐变量 z(i) 表示样本 X(i) 的类别;
w(i) 表示给定样本 i 下,其隐变量 z(i) =k 即其类别属于类别 k 的概率; k
b) 明确参数
α 表示所有样本在各个类别上的概率;
φ(l) 表示样本的第 l 个特征维度上:类别 j 取值为 m 的概率; jm
c) 计算隐变量条件概率分布 w(i) =
P(z(i) = j|x(i);θ)
P(z(i) = j|x(i);α,φ1,2...L)
P(x(i)|z(i) = j;α,φ1,2...L)∗P(z(i) = j;α) ∑Kj=1 P(x(i)|z(i) = j;α,φ1,2...L)∗P(z(i) = j;α)
j
=
P(X|θ)
logP(X,Z|θ)
= P(x i =1 j =1
∏N ∑K
= P(x i =1 j =1
,z
( i ) ( i )
( i )
= j|θ)
= =
 ∏L φ(l) ∗α l=1 jx(i)
l ∑K (∏L φ(l)
j=1 l=1 jx(i) l
j
∗α ) j
 d) 似然函数
两边同取对数:
∏N∑K (i)(i)
= j|θ)
= j,θ)∗P(z
∑N ∑K (i)(i) = log P(x |z
i=1 j=1
= j,θ)∗P(z
(i)
= j|θ)
|z
2
 e) 由c)和d)得Q(θ,θ)函数
  Q(θ,θ) = =
∑N ∑K
i =1 j =1
( i )
= j|x
( i ) ( i ) ;θ)∗logP(x
|z
( i )
= j,θ)∗P(z
( i )
= j|θ)
P(z
wj ∗log( φjx(i) ∗αj)
∑N∑K(i) ∏L(l)
i=1 j=1 利用拉格朗日乘子法对参数求导,分别获得 α j 和 φ(l ) 的求解公式:
αj= (i=1) j ,j=1,2,...,K N
∑N {w(i) x(i)=m φ(l)= j l
jm (i=1) 0 x(i) ̸=m l
l=1 l
jm ∑N w(i)
 注:需要对 φ(l) 按行归一化,表示样本的第 l 个特征维度上:类别 j 在各个取值 m 上的概 率分布。
f) 反复执行 c) 和 e) 步,直到模型收敛。 第二部分:实验过程与结果分析
1. 数据预处理
• 数据集分为 15 个文件,取第一个文件 1.csv 处理数据; • 类别 0 只有一个样本,将其作为异常点删除;
• 离散化处理各属性;
• 划分数据为 80% 训练集,20% 验证集。
2. 实验过程
我们目标是利用 EM 算法进行样本聚类。
a) 推导并实现朴素贝叶斯非监督聚类下的 EM 算法;
b) 分析聚类个数对聚类效果影响。
c) 将聚类结果作为新特征输入,分别分析对朴素贝叶斯(多项式)、朴素贝叶斯(高斯)、 Logistic 回归分类器的影响;
注:因为聚类结果没有顺序,所以对于每一个聚类类别,我们将在此聚类中出现实际类别最多的 类作为其分类;如聚类 1 中出现的实际类的个数为:1 类别 10 个,2 类别 20 个,3 类别 0 个, 4 类别 50 个,那么我们认为此聚类类别 1 代表实际类别 4。
3. 实验结果
我们采用 300 轮迭代充分收敛的聚类精度作为分析。
 普通分类器
训练集精度
测试集精度
朴素贝叶斯-多项式 68.28% 68.29%
朴素贝叶斯-高斯 77.64% 77.62%
Logistic 回归 72.29% 72.25%
  其中朴素贝叶斯-高斯分类器精度最高。
下面我们列出 EM 算法在不同聚类个数上精度效果;同时将聚类结果作为新特征输入贝叶斯-多 项式、贝叶斯-高斯、Logistic 回归模型,分析新特征对这些模型的影响:
3
 EM 训练集 - EM 测试集 -
Logistic 训练集 72.29% Logistic 测试集 72.25%
朴素贝叶斯-多项式 训练集 68.28% 朴素贝叶斯-多项式 测试集 68.29%
朴素贝叶斯-高斯 训练集 77.64% 朴素贝叶斯-高斯 测试集 77.62%
现象与分析:
73.78% 75.72% 73.75% 75.54%
74.39% 73.76% 74.28% 73.54%
67.52% 70.72% 67.7% 69.68%
74.76% 75.93% 74.78% 75.72%
初始精度 聚类7 聚类30
聚类80 76.46% 76.11%
73.24% 73.14%
61.62% 61.86%
76.61% 76.41%
        (1)EM 算法聚类精度较高,证明了我们算法的正确性。同时随着聚类个数越多,精度越高的现 象,我们猜测是聚类个数多,将误差进行了分散(与我们计算精度方式有关)。
(2)对于朴素贝叶斯方法(多项式、高斯),对特征较为敏感,新增加一些有用的特征,效果并 不一定会有所提升(这意味着对这两种模型,堆特征的方式一般是不可行的)。
(3)对于 Logistic 回归模型,我们发现将聚类个数为 7、30、80 的情况下的聚类结果作为新增 维度的参数,精度都有提升,此模型对新特征的接纳程度较高(工业界常用 Logistic+ 堆特征的 方式也可以从侧面证明这一点);但是由增加精度幅度不同,我们也可以看出,对于 Logistic 回 归,我们增加特征也是需要谨慎的。
注:因为 EM 算法随机初始化不同,所以结果会略有出入。
4. 模型评价
我们实现了贝叶斯无监督情况下的聚类算法,结果基本验证了我们算法和实现的正确性;对比 了不同聚类个数对精度的影响;与朴素贝叶斯与 Logistic 回归等模型的实验结果对比表明效果 可以逼近普通分类器。
5. 收获
a) 初步使用 EM 算法进行聚类,与其它分类器精度的比较表明效果达到预期目标;
b) 聚类个数为 7、30、80 情况下,测试集准确率分别为:73.75%、75.54%、76.11%,聚类个 数增多,对分类结果精度有帮助(我们认为聚类个数增多将分类误差分散,使得更容易分 正确)。
4
